{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECOMMERCE ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction -\n",
    "\n",
    "In this notebook we examine the ecommerce dataset. We start with (a) finding feature types, (b) missing values, (c) creating new feature from existing dataset, (d) feature analysis and visualization of the data using plots and (e) using plot to answer hypotheses and relevant questions.\n",
    "\n",
    "I have made duplicate charts using plotly express to showcase the interactivity and ease of using plotly as compared to seaborn/matplotlib\n",
    "\n",
    "### 2. Impetus - \n",
    "\n",
    "We are interested in analysing the sales of ecommerce in todays digital age. Taking the sample set of data, we would like to analysis the trend of ecommerce and to see if geography, month of the year, time of the day play a part in online spending."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Summary of dataset - \n",
    "This is a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail. a) The company mainly sells unique all-occasion gifts b) Many customers of the company are wholesalers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONTENTS\n",
    "\n",
    "Section 1: Data Preparation\n",
    "\n",
    "Section 2: Data Exploration\n",
    "\n",
    "Section 3: Data Augmentation\n",
    "\n",
    "Section 4: Hypotheses\n",
    "\n",
    "Section 5: Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Section 1: Data Preparation</u>\n",
    "\n",
    "1. Firstly, import all the necessary libraries that are required for the data analaysis\n",
    "2. Explore the shape, columns and dataset to get a feel on what kind of features and the volume of data to handle (info) (total row 541909, total 8 columns) \n",
    "3. Find if there are any null data in the dataset. There are 1454 null data in \"Description\" and 135080 null Data in Customer ID <br>\n",
    "    a) Determine how much % of the missing data as compared to the whole dataset, plot it. <br>\n",
    "    b) For demo sake, dropped all the missing data from the dataset<br>\n",
    "    c) Remove duplicate rows in dataset<br>\n",
    "4. Determine how many unique country and what are they"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the required library/modules\n",
    "import anvil.mpl_util\n",
    "import anvil.server\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import calendar\n",
    "\n",
    "import json\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "import warnings\n",
    "import plotly.express as px\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "data=pd.read_csv(\"ecommerce.csv\", encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Explortatory \n",
    "print(data.head())                #see the first 5 rows of data\n",
    "print(data.info())                #see the no. of columns,row, the column names and data types\n",
    "print(data['Country'].value_counts())  # check how many transactions per country\n",
    "print(data.Country.unique())      #determine the different countries in the data set\n",
    "print(data.Country.nunique())     #count the total different countries in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the missing data percentage, and plot it. Visualise how many % of missing value as of the whole dataset\n",
    "plt.style.use('default')\n",
    "total=data.isnull().sum()\n",
    "percent=data.isnull().sum()/data.isnull().count()*100\n",
    "missing_data=pd.concat([total,percent],axis=1, keys=['total', 'percent'])\n",
    "ax = plt.subplots(figsize=(12, 6))\n",
    "plt.xticks(rotation='40')\n",
    "sns.barplot(x=missing_data.index,y=missing_data['percent'])\n",
    "plt.xlabel('Columns', fontsize=15)\n",
    "plt.ylabel('Percent of missing values', fontsize=15)\n",
    "plt.title('Percent missing data by feature', fontsize=15)\n",
    "plt.savefig('Pic1_Percent_missing_values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.barplot(x=missing_data.index,y=missing_data['percent'])\n",
    "px.bar(x=missing_data.index, y=missing_data.percent, title=\"Missing Data Viz\", labels={\"x\": \"Features\",\"y\":\"Percentage\"} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.dropna(inplace=True)    #drop all rows with empty value, for simplicity\n",
    "# data.duplicated().sum()      #check for duplicated rows and to drop if necessary\n",
    "# data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Section 2: Data Exploration</u>\n",
    "\n",
    "1. Create a Total_price column, which will be used for revenue analaysis\n",
    "2. Breakdown InvoiceDate into Date and time column\n",
    "3. Group country, year and time data to allow plots to answer varies query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Total_price\"]=data[\"Quantity\"]*data[\"UnitPrice\"]  #create new column of total price, used for revenue \n",
    "data[\"Datetime\"]=pd.to_datetime(data[\"InvoiceDate\"]) #use pandas datetime method to extract Year, Month and Day\n",
    "data[\"Year\"]=data[\"Datetime\"].dt.year\n",
    "data[\"Month\"]=data[\"Datetime\"].dt.month\n",
    "data[\"Day\"]=data[\"Datetime\"].dt.day\n",
    "data[\"Hour\"]=data[\"Datetime\"].dt.hour\n",
    "\n",
    "print(data.Datetime.min())     #the first transaction of the dataset : 2010-12-01 08:26:00\n",
    "print(data.Datetime.max())     #the last transaction of the dataset:   2011-12-09 12:50:00\n",
    "##Grouping Datapoints##\n",
    "\n",
    "data = data.astype({\"Year\": str, \"Month\": int,\"Hour\": int, \"Day\": str}, errors='raise') \n",
    "\n",
    "#Determine total revenue recorded base on each country\n",
    "country_sum=data.groupby([\"Country\"]).agg(total_revenue = (\"Total_price\", \"sum\")) \n",
    "country_sum=country_sum.reset_index()\n",
    "print(country_sum.sort_values(ascending=False, by=\"total_revenue\").head(5))\n",
    "display(country_sum)\n",
    "\n",
    "yearly_sales=data.groupby([\"Year\",\"Month\"]).agg(total_revenue = (\"Total_price\", \"sum\")) \n",
    "yearly_sales=yearly_sales.reset_index()\n",
    "display(yearly_sales)\n",
    "\n",
    "#Grouping the hourly transaction\n",
    "hourly_transaction_count=data.groupby([\"Hour\"]).agg(Total_transaction= (\"InvoiceNo\", \"count\")) \n",
    "hourly_transaction_count=hourly_transaction_count.reset_index()\n",
    "display(hourly_transaction_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.StockCode.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Description.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.CustomerID.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1) Which month of the Year has the most sales?\n",
    "September and November seems to have the highest sales for the year. It may be because wholeseller are stocking up on their inventory so as to handle the high sales period in festive month of October (Halloween) and December (Christmas). <br>\n",
    "\n",
    "Dec 2011 sales seems low as the data was captured up to 10 Dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15)) # this creates a figure 20 inch wide, 15 inch high\n",
    "plt.tight_layout()\n",
    "plt.title('Comparision of Sales Data between months', fontsize=30)\n",
    "#Creating bar plot to see sales between two year\n",
    "ax=sns.barplot(x=\"Month\", y=\"total_revenue\", data=yearly_sales, hue=\"Year\")\n",
    "ax.legend(fontsize=30, loc='upper left')\n",
    "\n",
    "ax.set_xlabel(\"Month\",fontsize = 20)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\", fontsize = 18)\n",
    "\n",
    "ax.set_ylabel(\"Total Revenue\", fontsize = 20)\n",
    "ax.set_yticklabels(ax.get_yticklabels(), rotation=40, ha=\"right\", fontsize = 18)\n",
    "ax=ax.set(yscale=\"log\")\n",
    "#plt.savefig('Pic3_Compare_month_sales_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_sales.info()\n",
    "#We convert yearly_sales from Int to Str, else plotly will treat it as a continuous value which we do not want.\n",
    "yearly_sales = yearly_sales.astype({\"Year\": str}, errors='raise') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_sales.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax=sns.barplot(x=\"Month\", y=\"total_revenue\", data=yearly_sales, hue=\"Year\")\n",
    "fig=px.bar(yearly_sales,x=\"Month\", y=\"total_revenue\", color=\"Year\",barmode='stack', text_auto=\".2s\", title=\"Comparision of Sales Data between months\")\n",
    "fig.update_layout(        \n",
    "        xaxis = dict(\n",
    "        tickmode = 'linear',\n",
    "        tick0 = 2017,\n",
    "        dtick = 1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2) Which country has the most sales? \n",
    "Ecommerce app is based in United kingdom, mostly targeting local market and neighbouring country and Europe. This is supported by the data which show the highest sales in UK, which are then followed by neighbouring Europe country (Ireland, Netherlands, France, Germany). <br> <br> Interestingly, we see a high sales figure in Australia, which is far away from Europe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15)) # this creates a figure 15 inch wide, 15 inch high\n",
    "plt.tight_layout()\n",
    "plt.title('Revenue by Country', fontsize=30)\n",
    "ax = sns.barplot(x=\"Country\", y=\"total_revenue\", data=country_sum)\n",
    "\n",
    "ax.set_xlabel(\"Country\",fontsize = 20)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\", fontsize = 14)\n",
    "\n",
    "\n",
    "ax.set_ylabel(\"Total Revenue\", fontsize = 20)\n",
    "ax.set_yticklabels(ax.get_yticklabels(), rotation=40, ha=\"right\", fontsize = 18)\n",
    "ax=ax.set(yscale=\"log\")\n",
    "plt.savefig('Pic4_Compare_Country_revenue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax = sns.barplot(x=\"Country\", y=\"total_revenue\", data=country_sum)\n",
    "fig=px.bar(country_sum, x=\"Country\", y=\"total_revenue\",log_y=True, title=\"Total Revenue by Country\",text_auto=\".2s\")\n",
    "fig.update_layout(barmode='stack')\n",
    "fig.update_xaxes(categoryorder='total descending')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3) What hour of the day have the most sales?\n",
    "It is noticed that most of the sales happenned during working hours, with the peak at noon. <br> Hourly sales is important, as we can advise on targeted ads during high traffic hours to save on marketing cost. <br>\n",
    "There is no sales data during the period from 2000H to 0600H, which further fortify the fact that most, if not all, of the client are wholesalers who do purchasing during working hours. Or it can be as simple as that the e-commerce do no process transaction from 2000H to 0600H."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hourly_transaction_count\n",
    "plt.title('Heatmap of total transactions across the day', fontsize=15)\n",
    "heatmap1_data=pd.pivot_table(data=hourly_transaction_count, index= [\"Hour\"])\n",
    "sns.color_palette(\"rocket\", as_cmap=True)\n",
    "ax=sns.heatmap(heatmap1_data)\n",
    "ax=ax.set_yticklabels(ax.get_yticklabels(), rotation=45, ha=\"right\", fontsize =10)\n",
    "#plt.savefig('Pic5_Transactions_acrosstheday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap1_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax=sns.heatmap(heatmap1_data)\n",
    "px.imshow(heatmap1_data, title=\"Total transactions across the day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Section 3: Data Augmentation </u>\n",
    "\n",
    "After finding out the sales for the different countries, we would like to find out the sales performance based on the country region. This would allow for us to know the market penetration base on the country region, to allow the company to better cater for strategy to handle emerging markets. \n",
    "\n",
    "However, the dataset did not classify itself based on region. We will be augmenting the dataset by finding the online dataset which have classified the country based on region, merging them together using Country as the key. This will allow us to see the market penetration base on region.\n",
    "\n",
    "1) Extract the region dataset (https://meta.wikimedia.org/wiki/List_of_countries_by_regional_classification) and create a new dataframe (region)\n",
    "2) Make sure that all unique country in ecommerce dataset is named the same as the region dataset in wikipedia (in  this case, the unspecified country row will be dropped)\n",
    "3) Merge both dataset based on country\n",
    "4) Groupby region and against total revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_wiki_data=pd.read_html(\"https://meta.wikimedia.org/wiki/List_of_countries_by_regional_classification\")\n",
    "wiki_data=list_wiki_data[0]   #first table in the index from the website\n",
    "\n",
    "print(wiki_data.Region.nunique())   #8 regions in the world\n",
    "print(wiki_data.Region.unique())    #['Europe' 'Middle east' 'Asia & Pacific' 'South/Latin America' 'Africa' 'Arab States' 'North America' 'South/Central America']\n",
    "\n",
    "#Inserting rows into wiki dataset to ensure all country are represented \n",
    "display(wiki_data)\n",
    "df1 = {\"Country\":\"European Community\", \"Region\":\"Europe\", \"Global South\": \"Global South\"} #No EC in wiki data, to treat as a european country\n",
    "df2= {\"Country\":\"RSA\", \"Region\":\"Africa\", \"Global South\": \"Global South\"}   #RSA = South Africa in wiki dataset\n",
    "df3= {\"Country\":\"Channel Islands\", \"Region\":\"Europe\", \"Global South\": \"Global North\"} #No Channel Islands in wiki data, nearest to Europe, to treat as Europe country\n",
    "df4= {\"Country\":\"USA\", \"Region\":\"North America\", \"Global South\": \"Global North\"} #USA - United States in wiki dataset\n",
    "wiki_data=wiki_data.append(df1,ignore_index = True)\n",
    "wiki_data=wiki_data.append(df2,ignore_index = True)\n",
    "wiki_data=wiki_data.append(df3,ignore_index = True)\n",
    "wiki_data=wiki_data.append(df4,ignore_index = True)\n",
    "\n",
    "merged_country_region=pd.merge(country_sum,wiki_data)\n",
    "display(merged_country_region)\n",
    "merged_country_region.Region.unique()\n",
    "group_by_region=merged_country_region.groupby([\"Region\"]).agg(total_region_revenue = (\"total_revenue\", \"sum\"))\n",
    "group_by_region=group_by_region.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15)) # this creates a figure 20 inch wide, 15 inch high\n",
    "plt.tight_layout()\n",
    "plt.title('Revenue by Region', fontsize=30)\n",
    "ax = sns.barplot(x=\"Region\", y=\"total_region_revenue\", data=group_by_region)\n",
    "\n",
    "ax.set_xlabel(\"Region\",fontsize = 20)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\", fontsize = 15)\n",
    "\n",
    "ax.set_ylabel(\"Total Revenue\", fontsize = 20)\n",
    "ax.set_yticklabels(ax.get_yticklabels(), rotation=40, ha=\"right\", fontsize = 20)\n",
    "ax=ax.set(yscale=\"log\")\n",
    "plt.savefig('Pic6_Revenue_by_region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax = sns.barplot(x=\"Region\", y=\"total_region_revenue\", data=group_by_region)\n",
    "px.bar(group_by_region,x=\"Region\", y=\"total_region_revenue\",log_y=True, text_auto=\".2s\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=px.bar(merged_country_region, x=\"Country\", y=\"total_revenue\", color=\"Region\", log_y=True)\n",
    "fig.update_layout(barmode='stack')\n",
    "fig.update_xaxes(categoryorder='total descending')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.treemap(merged_country_region, path=[px.Constant(\"All\"), \"Region\",\"Country\"], values='total_revenue')\n",
    "                          #color_continuous_scale='RdBu')\n",
    "                          #color_continuous_midpoint=np.average(treedf['lifeExp'], weights=treedf['pop']))\n",
    "fig.update_layout(margin = dict(t=50, l=25, r=25, b=25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Metrics for Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "def Highest_revenue_country():\n",
    "    \"\"\" Return a tuple: the total gross revenue and total no. of orders for the dataset\n",
    "    \n",
    "    Parameters- Nil\n",
    "    Returns- Tuple of strings\n",
    "    \n",
    "    \"\"\"\n",
    "#     #calculate total revenue\n",
    "#     total_revenue= round(data[ (data[\"cancelled\"]== False)].ticket_revenue.sum())\n",
    "    \n",
    "#     #total number of orders   # cannot convert empty value to string # so fill na value to string\n",
    "#     df[\"comments\"].fillna(\"-\", inplace=True)  # replace original\n",
    "#     #df[\"comments\"]=df[\"comments\"].fillna(\"-\", inplace=True)   #\n",
    "#     total_num_orders= df[ (~df.comments.str.contains('Transferred to:')) & (df[\"cancelled\"] == False)].shape[0]\n",
    "    #Find Top performing country in terms of total revenue\n",
    "    tempdf=data.groupby([\"Year\",\"CustomerID\"], as_index=False)[\"Total_price\",\"InvoiceNo\"].agg({\n",
    "        \"Total_price\":\"sum\",\n",
    "        \"InvoiceNo\":\"count\"\n",
    "    })\n",
    "    tempcountry=data.groupby([\"Country\"], as_index=False)[\"Total_price\"].sum()\n",
    "    tempcountry.loc[tempcountry.Total_price.argmax()].Country\n",
    "    \n",
    "    return tempcountry.loc[tempcountry.Total_price.argmax()].Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "def top_revenue_product():\n",
    "    tempdf=data.groupby([\"StockCode\"], as_index=False)[\"Total_price\"].sum()\n",
    "    tempdf.loc[tempdf.Total_price.argmax()].StockCode\n",
    "    \n",
    "    return data[data.StockCode==tempdf.loc[tempdf.Total_price.argmax()].StockCode].Description.unique()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "def average_spent():\n",
    "    tempdf=data.groupby([\"CustomerID\"], as_index=False)[\"Total_price\"].sum()\n",
    "    #tempdf.loc[tempdf.Total_price.argmax()].StockCode\n",
    "    average_spend=tempdf.Total_price.sum()/tempdf.CustomerID.count()\n",
    "    return \"{:,.0f}\".format(average_spend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "def unique_customer():\n",
    "    return data.CustomerID.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "def revenue_by_country():\n",
    "    fig=px.bar(merged_country_region,x=\"Country\", y=\"total_revenue\", title=\"Revenue by Country\",color=\"Region\",log_y=True, text_auto=\".2s\",\n",
    "               labels=dict(Country=\"Country\", total_revenue=\"Revenue ($)- Log Scale\",Region=\"Region\"))\n",
    "    fig.update_xaxes(categoryorder='total descending', tickangle=45)\n",
    "    fig.update_yaxes(dtick=1)\n",
    "\n",
    "    fig.update_layout() \n",
    "    fig.show()\n",
    "    fig1=fig.to_json()\n",
    "    return fig1, json.loads(fig1)[\"data\"] , json.loads(fig1)[\"layout\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "def revenue_by_country_month(country):\n",
    "    #Return rows that reflect users selction of country, we only comparing for year 2011\n",
    "    filtered_country_df=data[(data.Country==country) & (data.Year==\"2011\")]\n",
    "    \n",
    "    temp_location_df=filtered_country_df.groupby([\"Month\"], as_index=False)[\"Total_price\",\"InvoiceNo\"].agg({\n",
    "        \"Total_price\":\"sum\",\n",
    "        \"InvoiceNo\": \"count\"})\n",
    "    print(temp_location_df)\n",
    "    fig=px.bar(temp_location_df,x=\"Month\", y=\"Total_price\", title=f\"{country} Revenue- 2011\")\n",
    "    fig.update_xaxes(range=[1, 12]) #Force X axes range to 1-12 month\n",
    "    fig.update_layout(       \n",
    "        xaxis = dict(\n",
    "        tickmode = 'linear',\n",
    "        tick0 = 2017,\n",
    "        dtick = 1)\n",
    "    )\n",
    "    fig.show()\n",
    "    fig1=fig.to_json()\n",
    "    return fig1, json.loads(fig1)[\"data\"] , json.loads(fig1)[\"layout\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "def heat_map():\n",
    "    fig=px.imshow(heatmap1_data, title=\"Total transactions across the day\")\n",
    "    fig.show()\n",
    "    fig1=fig.to_json()\n",
    "    return fig1, json.loads(fig1)[\"data\"] , json.loads(fig1)[\"layout\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inventory Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "def total_unique():\n",
    "    return data.StockCode.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "def popular_item():\n",
    "    most_sc=data.groupby([\"StockCode\"], as_index=False)[\"Quantity\"].sum().sort_values(by=\"Quantity\",ascending=False).StockCode.iloc[0]\n",
    "    most_name=data.Description[data[\"StockCode\"]==most_sc].iloc[0]\n",
    "    return f\"Stock Code: {most_sc} - {most_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "def least_popular_item():\n",
    "    least_sc=data.groupby([\"StockCode\"], as_index=False)[\"Quantity\"].sum().sort_values(by=\"Quantity\",ascending=True).StockCode.iloc[0]\n",
    "    least_name=data.Description[data[\"StockCode\"]==least_sc].iloc[0]\n",
    "    return f\"Stock Code: {least_sc} - {least_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "def populate_country():\n",
    "    return list(data[\"Country\"].unique().astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "def inventory(country,sortby):\n",
    "    \n",
    "    \"\"\" Return list of dictionary of strings: Revenue Table format based on different school \n",
    "    \n",
    "    Parameters- rows_to_display: int\n",
    "    Returns- list of dictionary of strings\n",
    "    \n",
    "    \"\"\"\n",
    "    filtered_country_df=data[(data.Country==country)]\n",
    "    temp_df=filtered_country_df.groupby([\"StockCode\",\"Description\"], as_index=False)[[\"Total_price\",\"InvoiceNo\"]].agg({\n",
    "        \"Total_price\":\"sum\",\n",
    "        \"InvoiceNo\": \"count\"})\n",
    "\n",
    "    #Do the necessarry sort here first. This will allow populating the datagrid in sorted sequence based on revenue\n",
    "    temp_df_sorted=temp_df.sort_values(by=sortby,ascending=False)\n",
    "    response=[]\n",
    "    counter=1\n",
    "    for index, row in temp_df_sorted.iterrows():\n",
    "        #print(row[\"order_year\"])\n",
    "        response.append({'s_n': str(counter),'stock_code': str(row[\"StockCode\"]),'description': str(row[\"Description\"]), \"total_transaction\":int(row[\"InvoiceNo\"]), \"total_revenue\": \"$ {:,.0f}\".format(row[\"Total_price\"])})\n",
    "        counter+=1\n",
    "    print(response)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "def tree_map():\n",
    "    fig = px.treemap(merged_country_region, path=[px.Constant(\"All\"), \"Region\",\"Country\"], values='total_revenue',title=\"Total Revenue by Region:\")\n",
    "    fig.show()\n",
    "    fig1=fig.to_json()\n",
    "    return fig1, json.loads(fig1)[\"data\"] , json.loads(fig1)[\"layout\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anvil.server.connect(\"I2DPNA2SQZUS6DTN53U3B7CX-X6DMB3KA4HQS3TB6\")\n",
    "anvil.server.wait_forever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah! With treemap chart, you can clearly see the dominance of Europe market and the low penetration for the other region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Section 4: Hypotheses</u>\n",
    "\n",
    "a. Hypothesis 1 - Superpower country have the highest spending in e-commerce company. For example, USA is one of the largest economy of the world. Therefore, we would like to find out if USA is also one of the top revenue generating country for the ecommerce company   <br>\n",
    "    a) This is proven false in Q3, as we can see that UK and their neighbouring countries generate much more sales. This, however, is a skewed analysis as the dataset is an e-commerce company that is based in the UK. <br><br>\n",
    "b. Hypothesis 2 - Festive season month (October- Halloween, December - Christmas) have the most sales for the ecommerce company. By determining if festive season have a correlation with total sales, we can better cater to logistic/manpower/marketing on the different month.  <br>\n",
    "    a) This is proven True in Q2. Most of the sales happened one month before the festive season month. This may be due to logistic lead time required for wholesaler to prepare for the higher demand in consumer purchase during festive season. <br> <br>\n",
    "c. Hypothesis 3 - The most sales for e-commerce company happens after working hours as people have time to shop online. Knowing when the \"golden hour\" is allows for cost effective marketing and additional income generation (targeted ads, charge more money to push retailer sales during the \"golden hour\") <br>\n",
    "    a) This is proven false in Q4. As most of the customer are wholesaler, it makes sense that their purchase happened during the working hours.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Section 5: Conclusion</u>\n",
    "\n",
    "In conclusion, we are able to do clean up the huge dataset and did data exploration to answer some interesting questions like the total sales by the different country and the hourly transaction sales data. We also augment the dataset from external source to better highlight which emerging markets the ecommerce is able to tackle. All these analaysis assist the ecommerce company to better allocate resources.\n",
    "\n",
    "What can be done better?\n",
    "\n",
    "Handling of missing data : After the data augmentation phase, we realise that Hong Kong did not registered any transaction. This is because all of Hong Kong transactions have no customer ID (therefore they are treated as invalid data and hence drop off from the total_revenue calcuation). We should work with the e-commerce company to better understand why these customerID are ommitted from the Hong Kong data\n",
    "\n",
    "Understanding the data and how it is formatted: We initially assumed that the Invoice date is formatted as DD/MM/YYYY (just like how we write dates in Singapore). However, the date in the dataset is formatted month first (MM/DD/YYYY). This cause us to process the data wrongly and give us a plot that may give wrong analysis. It is important to read through the dataset thoroughly and understand how it is formatted."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
